{"posts":[{"title":"我是谁","text":"我是Skywuuuu，本科就读于厦门大学，现在是上海交通大学的一名直博生，研究兴趣是Neurosymbolic和可解释性强化学习。","link":"/2023/06/21/hello/"},{"title":"chap3-有限马尔可夫决策过程","text":"知识（知识概要，公式推导，知识之间的联系等等）数学符号定义表示右边是左边的定义 背景随机变量 样本空间：包含一个实验的所有可能结果的集合。 随机变量的定义：定义在一个实验的样本空间中的实值函数。 P(X=x)指的是随机变量X等于某种情况x时的概率。 p(x) = P(X=x)中的p(x)是离散的随机变量X的概率质量函数（Probability Mass Function, PMF）当然还有连续的随机变量，不过这里就不多讲了，在ONENOTE中概率论的笔记中有记录。 ^95f50b X，Y等等-&gt;引出随机过程 随机过程 定义：一个集合，该集合收集了被时间或空间标记的随机变量。 记作： 例子：股票市场中在时刻t时，S(t)是一个随机变量；在时刻t+1时，S(t+1)也是一个随机变量…这里有点逆直觉，S(t)是和上面的X一样，被看为一个随机变量的。马尔可夫性质 为什么叫做马尔可夫性质呢？因为是俄国数学家马尔可夫想出来的，我们关注定义，不要关注名字。 定义：状态S(t+1)只取决于它的上一时刻的S(t)，而不取决于除了S(t)之外的其他历史时刻。也就是说，S(t+1)和S(t-1)…S(1)是独立的，但S(t+1)和S(t)不是独立的。（数学定义非常直观） 数学定义： 为什么要提出这个性质呢？主要是为了简化运算过程。其次在常识上我们也更可以这样思考，一个人十年前打篮球打不好，但他去年帮助球队拿到了某个篮球比赛的冠军，那么今年他的状态肯定跟去年关系比较大。马尔可夫过程（马尔可夫链） 定义：具有马尔可夫性质的随机过程 例子：比如S(1)=晴天，S(2)=下雨，S(3)=晴天，那么S(3)只和S(2)有关系 马尔可夫链可以被看作是一个的元组，S代表状态空间，P代表转移矩阵-&gt;后面的转移函数 转移矩阵定义：t时刻下，表示从状态转移到状态的概率，。 ^b5db88 马尔可夫过程-&gt;马尔可夫回报过程马尔可夫回报过程 简单理解：马尔可夫过程+回报 的元组，多了一个R代表回报，公式放在下面的马尔可夫决策过程讲 马尔可夫回报过程-&gt;马尔可夫决策过程马尔可夫决策过程初步认识在离散的时刻t=0，1，2，3…，智能体（agent）和环境（environment）都有交互。在每个时刻t，智能体观察到环境状态（state）的某种特征，这种特征表示为t时刻下的状态，其中，并且根据环境状态的特征选择了一个动作(action)，表示为t时刻下的动作，其中，在下一时刻，作为这个动作的结果，智能体得到了一个数值化的收益（reward），表示为，其中，并进入了下一个状态。从而，MDP和智能体共同给出了一个序列或轨迹（trajectory），类似数学描述给定前继状态和动作的值时，和这两个随机变量的特定值，和在t时刻出现的概率是 p(s',r|s,a) \\triangleq Pr\\{S_{t+1}=s',R_{t+1}=r|S_{t}=s,A_{t}=a\\}函数p是一个定义，而不是从之间定义推导出来的事实。p是动态函数（上面随机变量[[#^95f50b]]提到了p(x)是一个函数，这里的p和上面p是一个意思），p：是有四个参数的普通的确定性函数，它定义了MDP的动态特性[[#^ae43bb]]。p为每个s和a的选择都制定了一个概率分布 \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}}p(s', r|s, a) = 1对于所有和。和上面提到的状态转移矩阵[[#^b5db88]]大同小异，就是四个维度人类很难想象出来。这里反复强调一下，在马尔可夫决策过程中，和的每个可能出现的值的概率只取决于前一个状态和，并且与更早之前的状态和动作无关。由推导状态转移概率（我们将其表示为一个三参数函数p：） p(s'|s,a) \\triangleq \\sum_{r \\in \\mathcal{R}}p(s',r|s,a)还可以引出一些其他的公式，但最重要的还是这个四参数函数！！！价值函数目标和收益智能体的目标被形式化地表征为一种特殊的信号，收益。在每一个时刻，收益都是一个单一的标量数值，。智能体的目标就是要最大化总收益，也就是说，智能体要最大化的不是当前时刻的收益，而是长久以往地积累收益以期达到最大。这引出了收益假设： 我们所有的“目标”或“目的”都可以总结为：最大化智能体接收到的标量信号（称之为收益）累积和的概率期望值 所以我们在设计收益的时候，就需要注意我们设计的收益是否能引导智能体最大化总收益，从而实现我们设定的目标。案例：下围棋的时候，我们不能将收益设计为吃更多的子，因为就算吃更多的子可以获得当前最大收益，但最后还是可能输掉全局，获得比赛胜利才是我们的目标。因此，我们可以将收益仅仅确定为获得最终的胜利，而不要通过设计收益传授给智能体如何实现目标的先验知识。 回报和分幕每时每刻的收益是可以形成一个收益序列，，而我们希望能够最大化收益，所以我们寻求的是最大化期望回报，记为，从而有 G_t=R_{t+1}+R_{t+2}+ \\cdots +R_{T}其中T是代表最终时刻。这里就要引入一个幕（episode） 的概念了。 幕我们把智能体和环境交互的一系列子序列中的每一个子序列称为幕。游戏总是有结尾，下棋的输赢，机器人抓取物品是否成功等等。前一幕的结束跟这一幕的开始总是没有关系的，比如，前一盘比赛结束，就要重新开始一局新比赛，把棋盘清空，重新开始落子。具有上述分幕重复特性的任务也就是分幕式任务了。但是不是所有的情况下游戏都会有终点，比如我们的人生，人生的终点是死亡，但是每个人只会死一次，而这段时间非常非常非常长，这也就是一个持续性任务。 折扣率因此，对于持续性任务来说，中的，也就是说回报是无穷大的？这就不好说明了，所以我们引入一个额外概念，折扣率，来改进 G_t=R_{t+1}+\\gamma R_{t+2}+ \\gamma^2 R_{t+3} \\cdots +\\gamma^{T-t-1}R_{T}从式子可以看出折扣并不影响我们最大化收益，而且，我们可以发现，当时或，时刻越大，越小（时除了第一项，后面的项都为0），也就是说智能体会更注意最大化当前收益。但一般来说，最大化当前收益会减少未来的收益（先验知识）。因此，随着时，智能体就会更多地考虑未来收益。我们也可以很容易地推导出回报的迭代式： G_{t}=R_{t+1}+\\gamma G_{t+1}这个式子在最后推导贝尔曼方程很有用。我们还得注意一下这里的，它其实也是一个随机变量，这意味着t+1时刻的收益 策略和价值函数回溯树先把一个能帮助理解的回溯树放出来，后面会根据这个图玩转价值函数 策略 定义：从状态道每个动作的选择策略之间的映射。 数学表达： 确定性策略 vs 随机性策略 确定性策略：状态选择的行动是唯一的，也可以用表示。比如，状态只会在三个状态中选择 随机性策略：状态选择的行动是一个概率分布，用表示。比如，智能体在状态s下选中的概率是0.8，是0.1，是0.1 所以这样来看，确定性策略其实是随机性策略的一个特殊情况，也就是被选择的概率是1，其余两个动作被选择的概率是0。价值函数 由来：从上图中我们可以发现，如果智能体的策略是随机的，那么从状态转移到状态的情况就有9种（有3个动作供选择，选择一个动作后又可以跳转到3个不同的状态），还不考虑在状态之后的情况。因此，我们如果要计算，在状态下就有，很明显，我们不可能去耗费那么多时间计算，这样就引出了价值函数的定义。 ^47f3da 定义：价值函数就是回报的期望值。 我们发现价值函数与智能体所在的状态和智能体选择的动作都有关系，由此引出下面的函数。状态价值函数 定义：策略下状态的价值函数，即从状态开始，智能体按照策略进行决策所获得的回报的期望值。 数学定义式：v_{\\pi}(s)\\triangleq E_\\pi[G_t|S_t=s]=E_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}|S_t=s]动作价值函数 定义：策略下在状态时采取动作的价值，即根据策略，从状态开始，执行动作之后所有可能的决策序列的期望回报。 数学定义式：q_{\\pi}(s,a)\\triangleq E_\\pi[G_t|S_t=s,A_t=a]=E_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}|S_t=s,A_t=a]注意：状态价值函数与动作无关，而动作价值函数要在执行动作才能确定，所以就很自然的引出了二者的关系。状态价值函数和动作价值函数的关系状态价值函数回溯树动作价值函数回溯树（以动作为为例）通过观察，我们其实很容易发现，。状态价值函数其实就是对动作价值函数求期望。推广一下，二者的关系的数学表达式为 ^48ab05 v_{\\pi}(s)=\\sum_{a \\in A}\\pi(a|s)q_{\\pi}(a,s)贝尔曼方程的贝尔曼方程 ^70f6d4 我们直接从看图手写！ 先推和的关系，我们首先观察到的切换，在切换后，在t+1时刻的收益为r，而后在状态又有回报为，因此对于每个动作都有回报 G_{t}=r+\\gamma G_{t+1}由于就是回报的期望值，所以我们可以这样理解，同理，因为是状态的下一个状态，所以 从状态，通过动作，转移到了状态的概率，这个概念与最开始提到的一致 根据定义，表示执行了动作之后的期望收益，所以求和概率×回报有 ^05e8c1 \\begin{equation} \\begin{aligned} q_{\\pi}(a,s)&= \\sum_{s'\\in \\mathcal{S}}\\sum_{r \\in \\mathcal{R}}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]\\\\ \\end{aligned} \\end{equation} 代入到二者关系的式子，就能得到的贝尔曼方程（贝尔曼期望方程）了。$$\\begin{equation}\\begin{aligned}v_{\\pi}(s)&amp;=\\sum_{a \\in A}\\pi(a|s)q_{\\pi}(a,s)\\&amp;= \\sum_{a \\in A}\\pi(a|s)\\sum_{s’\\in \\mathcal{S}}\\sum_{r \\in \\mathcal{R}}p(s’,r|s,a)[r+\\gamma v_{\\pi}(s’)]\\ \\end{aligned}\\end{equation} ### $q_{\\pi}(s,a)$的贝尔曼方程 1. 有了$v_{\\pi}$的贝尔曼方程，那我们直接把它代入$v_{\\pi}(s)$与$q_{\\pi}(s,a)$的关系式[[#^48ab05]]，就可以得到$q_{\\pi}(s,a)$的贝尔曼方程 ## 贝尔曼最优方程 我们已经推导出了$v_{\\pi}(s)$和$q_{\\pi}(s,a)$的最优方程，但我们更关注的是要找出一个策略，使得强化学习任务能够在长期过程中最大化收益。对于有限MDP，我们可以通过比较价值函数精确地定义一个最优策略。如果对于所有$s \\in \\mathcal{S}, \\pi \\ge \\pi'$，那么应当有$v_{\\pi}(s) \\ge v_{\\pi}(s')$。总会存在至少一个不比别的策略差的策略，这个策略也就是最优策略。尽管最优策略可能不止有一个，但我们还是使用$\\pi_*$来表示所有这些最优策略。它们共享相同的状态价值函数，称之为最优状态价值函数，记作$v_*$，其定义为，对于任意$s\\in \\mathcal{S}$，v_*(s)\\doteq max_\\pi v_\\pi(s) 最优策略也共享相同的最优动作价值函数，记为$q_*$，其定义为，对于任意$s\\in \\mathcal{S}, a\\in \\mathcal{A}$,q_*(s,a)\\doteq max_\\pi q_\\pi(s,a) 上面这两个式子，也就是$v_*$和$q_*$又有什么关系呢？上面两个式子都是由最优策略$\\pi_*$引出，参考上文中的回溯树[[#^48ab05]]，我们可以发现最优策略下各个状态的价值一定等于这个状态下最优动作的期望回报，也就是v_(s)=max_{a} q_{\\pi_}(s,a) 这里其实引出一个问题，为什么最优策略下各个状态的价值一定等于这个状态下最优动作的期望回报呢？我们其实可以用反过来想，如果最优策略下各个状态的价值小于这个状态下最优动作的期望回报有可能吗？这也就是说会存在一个新策略，这个新策略比最优策略更优秀，可是我们定义最优策略使最优秀的了，这就产生了矛盾。因此，上面这个式子在逻辑上是成立的。 现在我们就可以来推导二者的贝尔曼最优方程了。 ### $v_*$的贝尔曼最优方程 我们参考上文的$v_\\pi$的贝尔曼最优方程v_\\pi(s)=\\sum_{a \\in A}\\pi(a|s)\\sum_{s’\\in \\mathcal{S}}\\sum_{r \\in \\mathcal{R}}p(s’,r|s,a)[r+\\gamma v_{\\pi}(s’)] 又因为最优策略下各个状态的价值一定等于这个状态下最优动作的期望回报，这里的最优动作其实暗含了该动作已经确定了的含义，所以$v_*$的贝尔曼最优方程为v_(s)=max_a\\sum_{s’\\in \\mathcal{S}}\\sum_{r \\in \\mathcal{R}}p(s’,r|s,a)[r+\\gamma v_{}(s’)] 注意，$p(s',r|s,a)$是仍然存在的，因为从状态$s$转移到下一个状态$s'$是一个概率问题，不是说我们选择了最优动作，就一定会跳转到固定的状态$s'$ ### $\\pi_*$的贝尔曼最优方程 我们参考上文的$q_{\\pi}(s,a)$的贝尔曼方程q_{\\pi}(s,a)= \\sum_{s’\\in \\mathcal{S}}\\sum_{r \\in \\mathcal{R}}p(s’,r|s,a)[r+\\gamma \\sum_{a’ \\in A}\\pi(a’|s’)q_{\\pi}(a’,s’)] 同上面推导$v_*$的贝尔曼最优方程一样，$\\pi_*$的贝尔曼最优方程为q_{}(s,a)= \\sum_{s’\\in \\mathcal{S}}\\sum_{r \\in \\mathcal{R}}p(s’,r|s,a)[r+\\gamma max_aq_{}(a’,s’)] $$ 部分可观察的马尔可夫决策过程 (POMDPSite Unreachable【强化学习RL】必须知道的基础概念和MDP - 水奈樾 - 博客园七元组$\\Omega是集合O是有条件的概率的集合，也就是O(o|s’,a)表示在状态s’采取动作a观测到o\\in \\Omega$的概率。 POMDP中，机器人不能直接确定自己处于哪个状态，只能讲，比如，在A状态的概率为0.8，在B状态的概率为0.1，在C状态的概率也为0.1。这其实就引出了belief的概念，机器人有多少的把握确定自己处在某个状态下就是belief，比如b(A)=0.8, b(B)=0.1, b(C)=0.1. 尽管如此，但是机器人可以通过某种动作，比如用传感器观察，来更新当前所处状态的可信度，也就是Observation的概念。 置信度马尔可夫决策过程 (Belief MDP) 引子：具有马尔可夫性质的置信度状态可以使我们将其看作为是一个以置信度为状态的MDP. 特点： Belief MDP一定是定义在连续的状态空间上的，因为即便是POMDP的状态空间有限，在某个状态上的概率分布是无限的，也就是说，置信度状态是无限的。 因为智能体总是会有一些概率相信自己处在某个状态上，所以对于置信度状态来说，它的动作是所有状态的所有动作的总集合。 置信度支持马尔可夫决策过程 (Belief Support MDP)belief support是一个集合，这个集合中包含了机器人有可能处在的状态，所以belief support MDP是有限的。 MDP for multi-agents给定N个智能体，多智能体的MDP为. 状态集合，其包含每个智能体的状态 思考，思维流程和收获思考马尔可夫决策过程中，智能体在当前时刻的状态下做出动作与环境交互，然后得到某种反馈，或者说是回报或奖励，并且进入到下一个时刻的状态。为什么强调是下一个时刻的状态呢？因为下一个时刻的状态可能与上一个时刻相同。 动态特性：我认为动态特性描述的是MDP中随着时间的变化，智能体不断做出动作，获得奖励并进入下一个状态，然后不断重复这几个步骤的特点 ^ae43bb 思维流程 [[#马尔可夫过程（马尔可夫链）]] -&gt; [[#马尔可夫回报过程]] -&gt; [[#马尔可夫决策过程]] -&gt; [[#]] 马尔可夫过程-&gt;马尔可夫回报过程-&gt;马尔可夫决策过程-&gt; POMDP-&gt; Belief MDP 最大化收益-&gt;回报的定义-&gt;回报会接近∞？-&gt;回报中加入折扣率-&gt;不同的状态下，选择不同的动作，回报都不一样，没法比较？-&gt;求回报的期望，也就是价值函数-&gt;状态价值函数，动作价值函数-&gt;二者关系-&gt;通过二者关系找到状态和状态的关系以及动作和动作的关系？-&gt;贝尔曼期望方程-&gt;寻求最优策略？-&gt;最优策略共享相同的两个贝尔曼最优方程我想不通或曾经想不通的地方 为什么和的关系的公式在《强化学习》和《深入浅出强化学习》这两本书中表达的方式不一样？这两种表达方式肯定都是对的，但是代表的是什么含义呢？ 为什么也要对r求期望？ 其实下图中的只画了一种情况，也是一个随机变量，它的取值是由不同的r决定的，其实从到的线应该有很多个不同取值的数。因此，就和上文提到的[[#^47f3da]]一样，我们也要对r求期望。这同时也回答了问题一。表达不同的地方就是《深入浅出强化学习》用代表求了期望之后的r，也就是，剩下的用了求和状态转移概率×价值函数得到，即","link":"/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"}],"tags":[],"categories":[{"name":"Personal","slug":"Personal","link":"/categories/Personal/"}],"pages":[]}