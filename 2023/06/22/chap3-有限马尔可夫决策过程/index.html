<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Skywuuuu&#039;s personal website</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Skywuuuu&#039;s personal website"><meta name="msapplication-TileImage" content="/img/avatar.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Skywuuuu&#039;s personal website"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="知识（知识概要，公式推导，知识之间的联系等等）数学符号定义$\triangleq$表示右边是左边的定义 背景随机变量 样本空间：包含一个实验的所有可能结果的集合。 随机变量的定义：定义在一个实验的样本空间中的实值函数。 P(X&amp;#x3D;x)指的是随机变量X等于某种情况x时的概率。 p(x) &amp;#x3D; P(X&amp;#x3D;x)中的p(x)是离散的随机变量X的概率质量函数（Probability Mass Function,"><meta property="og:type" content="blog"><meta property="og:title" content="Skywuuuu&#039;s personal website"><meta property="og:url" content="https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"><meta property="og:site_name" content="Skywuuuu&#039;s personal website"><meta property="og:description" content="知识（知识概要，公式推导，知识之间的联系等等）数学符号定义$\triangleq$表示右边是左边的定义 背景随机变量 样本空间：包含一个实验的所有可能结果的集合。 随机变量的定义：定义在一个实验的样本空间中的实值函数。 P(X&amp;#x3D;x)指的是随机变量X等于某种情况x时的概率。 p(x) &amp;#x3D; P(X&amp;#x3D;x)中的p(x)是离散的随机变量X的概率质量函数（Probability Mass Function,"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/马尔可夫链例一.jpg"><meta property="og:image" content="https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/强化学习基本功修炼/chap5-蒙特卡洛方法.assets/智能体与环境MDP的交互示意图.png"><meta property="og:image" content="https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/回溯树.jpg"><meta property="og:image" content="https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/状态价值函数回溯树..jpg"><meta property="og:image" content="https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/动作价值函数回溯树.jpg"><meta property="og:image" content="https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/comparison-table.png"><meta property="og:image" content="https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/回溯树.jpg"><meta property="article:published_time" content="2023-06-22T01:53:38.572Z"><meta property="article:modified_time" content="2023-06-22T01:53:38.573Z"><meta property="article:author" content="Skywuuuu"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/马尔可夫链例一.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"},"headline":"Skywuuuu's personal website","image":["https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/马尔可夫链例一.jpg","https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/强化学习基本功修炼/chap5-蒙特卡洛方法.assets/智能体与环境MDP的交互示意图.png","https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/回溯树.jpg","https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/状态价值函数回溯树..jpg","https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/动作价值函数回溯树.jpg","https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/comparison-table.png","https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/回溯树.jpg"],"datePublished":"2023-06-22T01:53:38.572Z","dateModified":"2023-06-22T01:53:38.573Z","author":{"@type":"Person","name":"Skywuuuu"},"publisher":{"@type":"Organization","name":"Skywuuuu's personal website","logo":{"@type":"ImageObject","url":"https://skywuuuu.github.io/img/avatar.jpg"}},"description":"知识（知识概要，公式推导，知识之间的联系等等）数学符号定义$\\triangleq$表示右边是左边的定义 背景随机变量 样本空间：包含一个实验的所有可能结果的集合。 随机变量的定义：定义在一个实验的样本空间中的实值函数。 P(X&#x3D;x)指的是随机变量X等于某种情况x时的概率。 p(x) &#x3D; P(X&#x3D;x)中的p(x)是离散的随机变量X的概率质量函数（Probability Mass Function,"}</script><link rel="canonical" href="https://skywuuuu.github.io/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"><link rel="icon" href="/img/avatar.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/avatar.jpg" alt="Skywuuuu&#039;s personal website" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Skywuuuu"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-06-22T01:53:38.572Z" title="6/22/2023, 9:53:38 AM">2023-06-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-06-22T01:53:38.573Z" title="6/22/2023, 9:53:38 AM">2023-06-22</time></span><span class="level-item">40 minutes read (About 5925 words)</span></div></div><div class="content"><h1 id="知识（知识概要，公式推导，知识之间的联系等等）"><a href="#知识（知识概要，公式推导，知识之间的联系等等）" class="headerlink" title="知识（知识概要，公式推导，知识之间的联系等等）"></a>知识（知识概要，公式推导，知识之间的联系等等）</h1><h1 id="数学符号定义"><a href="#数学符号定义" class="headerlink" title="数学符号定义"></a>数学符号定义</h1><p>$\triangleq$表示右边是左边的定义</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h3><ul>
<li>样本空间：包含一个实验的所有可能结果的集合。</li>
<li>随机变量的定义：定义在一个实验的样本空间中的实值函数。</li>
<li>P(X=x)指的是随机变量X等于某种情况x时的概率。</li>
<li>p(x) = P(X=x)中的p(x)是<strong>离散的</strong>随机变量X的概率质量<strong>函数</strong>（Probability Mass Function, PMF）当然还有连续的随机变量，不过这里就不多讲了，在ONENOTE中概率论的笔记中有记录。 ^95f50b</li>
</ul>
<p>X，Y等等-&gt;引出随机过程</p>
<h3 id="随机过程"><a href="#随机过程" class="headerlink" title="随机过程"></a>随机过程</h3><ul>
<li>定义：一个集合，该集合收集了被时间或空间标记的随机变量。</li>
<li>记作：$\lbrace S_t \rbrace ^{\infty}_{t=1}$</li>
<li>例子：股票市场中在时刻<strong>t</strong>时，S(t)是一个随机变量；在时刻<strong>t+1</strong>时，S(t+1)也是一个随机变量…这里有点逆直觉，S(t)是和上面的X一样，被看为一个随机变量的。<h3 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h3></li>
<li>为什么叫做马尔可夫性质呢？因为是俄国数学家马尔可夫想出来的，我们关注定义，不要关注名字。</li>
<li>定义：状态S(t+1)只取决于它的上一时刻的S(t)，而不取决于除了S(t)之外的其他历史时刻。也就是说，S(t+1)和S(t-1)…S(1)是独立的，但S(t+1)和S(t)不是独立的。（数学定义非常直观）</li>
<li>数学定义：$P(S_{t+1}|S_{t}, S_{t-1}, \dots S{1}) = P(S_{t+1}|S_{t})$</li>
<li>为什么要提出这个性质呢？主要是为了简化运算过程。其次在常识上我们也更可以这样思考，一个人十年前打篮球打不好，但他去年帮助球队拿到了某个篮球比赛的冠军，那么今年他的状态肯定跟去年关系比较大。<h3 id="马尔可夫过程（马尔可夫链）"><a href="#马尔可夫过程（马尔可夫链）" class="headerlink" title="马尔可夫过程（马尔可夫链）"></a>马尔可夫过程（马尔可夫链）</h3></li>
<li>定义：具有马尔可夫性质的随机过程</li>
<li>例子：比如S(1)=晴天，S(2)=下雨，S(3)=晴天，那么S(3)只和S(2)有关系<img src="马尔可夫链例一.jpg" alt="马尔可夫链例一"></li>
<li>马尔可夫链可以被看作是一个<S, P>的元组，S代表状态空间，P代表转移矩阵-&gt;后面的转移函数</li>
<li>转移矩阵定义：t时刻下，$p_{ij}$表示从状态$S_t(i)$转移到状态$S_{t+1}(j)$的概率，$\sum_{i=1}^{N}p_{ij}=1$。 ^b5db88</li>
<li>马尔可夫过程-&gt;马尔可夫回报过程<h3 id="马尔可夫回报过程"><a href="#马尔可夫回报过程" class="headerlink" title="马尔可夫回报过程"></a>马尔可夫回报过程</h3></li>
<li>简单理解：马尔可夫过程+回报</li>
<li><S, R, P>的元组，多了一个R代表回报，公式放在下面的马尔可夫决策过程讲</li>
<li>马尔可夫回报过程-&gt;马尔可夫决策过程<h2 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><h3 id="初步认识"><a href="#初步认识" class="headerlink" title="初步认识"></a>初步认识</h3>在离散的时刻t=0，1，2，3…，智能体（agent）和环境（environment）都有交互。在每个时刻t，智能体观察到环境状态（state）的某种特征，这种特征表示为t时刻下的状态$S_t$，其中$S_t \in \mathcal{S}$，并且根据环境状态的特征选择了一个动作(action)，表示为t时刻下的动作$A_t$，其中$A_t \in \mathcal{A}$，在下一时刻，作为这个动作的结果，智能体得到了一个数值化的收益（reward），表示为$R_{t+1}$，其中$R_{t+1} \in \mathcal{R}$，并进入了下一个状态$S_{t+1}$。从而，MDP和智能体共同给出了一个序列或轨迹（trajectory），类似$S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3, \dots$<br><img src="强化学习基本功修炼/chap5-蒙特卡洛方法.assets/智能体与环境MDP的交互示意图.png" alt="智能体与环境MDP的交互示意图"><h4 id="数学描述"><a href="#数学描述" class="headerlink" title="数学描述"></a>数学描述</h4>给定前继状态和动作的值时，$S_{t+1}$和$R_{t+1}$这两个随机变量的特定值，$s’ \in \mathcal{S}$和$r\in \mathcal{R}$在t时刻出现的概率是<script type="math/tex; mode=display">
p(s',r|s,a) \triangleq Pr\{S_{t+1}=s',R_{t+1}=r|S_{t}=s,A_{t}=a\}</script>函数p是一个定义，而不是从之间定义推导出来的事实。p是动态函数（上面随机变量[[#^95f50b]]提到了p(x)是一个函数，这里的p和上面p是一个意思），p：$\mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$是有<strong>四个参数</strong>的普通的确定性函数，它定义了MDP的动态特性[[#^ae43bb]]。p为每个s和a的选择都制定了一个概率分布<script type="math/tex; mode=display">
\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}}p(s', r|s, a) = 1</script>对于所有$s \in \mathcal{S}$和$r \in \mathcal{R}$。<br>和上面提到的状态转移矩阵[[#^b5db88]]大同小异，就是四个维度人类很难想象出来。这里反复强调一下，<strong>在马尔可夫决策过程中，$s_{t+1}$和$r_{t+1}$的每个可能出现的值的概率只取决于前一个状态$s_{t}$和$a_{t}$，并且与更早之前的状态和动作无关。</strong><h4 id="由-p-s’-r-s-a-推导"><a href="#由-p-s’-r-s-a-推导" class="headerlink" title="由$p(s’,r|s,a)$推导"></a>由$p(s’,r|s,a)$推导</h4>状态转移概率（我们将其表示为一个三参数函数p：$\mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$）<script type="math/tex; mode=display">
p(s'|s,a) \triangleq \sum_{r \in \mathcal{R}}p(s',r|s,a)</script>还可以引出一些其他的公式，但最重要的还是$p(s’,r|s,a)$这个四参数函数！！！<h2 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h2><h3 id="目标和收益"><a href="#目标和收益" class="headerlink" title="目标和收益"></a>目标和收益</h3>智能体的<strong>目标</strong>被形式化地表征为一种特殊的信号，<strong>收益</strong>。在每一个时刻，收益都是一个单一的标量数值，$R_t \in \mathcal{R}$。<br>智能体的目标就是要最大化总收益，也就是说，智能体要最大化的不是当前时刻的收益，而是长久以往地积累收益以期达到最大。这引出了收益假设：<blockquote>
<p>我们所有的“目标”或“目的”都可以总结为：最大化智能体接收到的标量信号（称之为收益）<strong>累积和</strong>的<strong>概率期望值</strong></p>
</blockquote>
</li>
</ul>
<p>所以我们在设计收益的时候，就需要注意我们设计的收益是否能引导智能体最大化总收益，从而实现我们设定的目标。<br>案例：下围棋的时候，我们不能将收益设计为吃更多的子，因为就算吃更多的子可以获得当前最大收益，但最后还是可能输掉全局，获得比赛胜利才是我们的目标。因此，我们可以将收益仅仅确定为获得最终的胜利，而不要通过设计收益传授给智能体如何实现目标的先验知识。</p>
<h3 id="回报和分幕"><a href="#回报和分幕" class="headerlink" title="回报和分幕"></a>回报和分幕</h3><p>每时每刻的收益是可以形成一个收益序列，$R_t, R_{t+1}, \dots, R_T$，而我们希望能够最大化收益，所以我们寻求的是最大化期望回报，记为$G_t$，从而有</p>
<script type="math/tex; mode=display">
G_t=R_{t+1}+R_{t+2}+ \cdots +R_{T}</script><p>其中T是代表最终时刻。这里就要引入一个<strong>幕（episode）</strong> 的概念了。</p>
<h4 id="幕"><a href="#幕" class="headerlink" title="幕"></a>幕</h4><p>我们把智能体和环境交互的一系列子序列中的每一个子序列称为幕。游戏总是有结尾，下棋的输赢，机器人抓取物品是否成功等等。前一幕的结束跟这一幕的开始总是没有关系的，比如，前一盘比赛结束，就要重新开始一局新比赛，把棋盘清空，重新开始落子。具有上述分幕重复特性的任务也就是<strong>分幕式任务</strong>了。<br>但是不是所有的情况下游戏都会有终点，比如我们的人生，人生的终点是死亡，但是每个人只会死一次，而这段时间非常非常非常长，这也就是一个<strong>持续性任务</strong>。</p>
<h4 id="折扣率"><a href="#折扣率" class="headerlink" title="折扣率"></a>折扣率</h4><p>因此，对于持续性任务来说，$G_t$中的$T=\infty$，也就是说回报是无穷大的？这就不好说明了，所以我们引入一个额外概念，折扣率$\gamma$，来改进$G_t$</p>
<script type="math/tex; mode=display">
G_t=R_{t+1}+\gamma R_{t+2}+ \gamma^2 R_{t+3} \cdots +\gamma^{T-t-1}R_{T}</script><p>从式子可以看出折扣并不影响我们最大化收益，而且，我们可以发现，当$\gamma \rightarrow 0$时或$\gamma=0$，时刻$x$越大，$\gamma^{x-t-1}R_{x}$越小（$\gamma=0$时除了第一项，后面的项都为0），也就是说智能体会更注意最大化当前收益。<strong>但一般来说，最大化当前收益会减少未来的收益</strong>（先验知识）。因此，随着$\gamma \rightarrow 1$时，智能体就会更多地考虑未来收益。<br>我们也可以很容易地推导出回报的迭代式：</p>
<script type="math/tex; mode=display">
G_{t}=R_{t+1}+\gamma G_{t+1}</script><p>这个式子在最后推导贝尔曼方程很有用。<br>我们还得注意一下这里的$R_{t+1}$，它其实也是一个随机变量，这意味着t+1时刻的收益</p>
<h3 id="策略和价值函数"><a href="#策略和价值函数" class="headerlink" title="策略和价值函数"></a>策略和价值函数</h3><h3 id="回溯树"><a href="#回溯树" class="headerlink" title="回溯树"></a>回溯树</h3><p>先把一个能帮助理解的回溯树放出来，后面会根据这个图玩转价值函数<br><img src="回溯树.jpg" alt="回溯树.jpg"></p>
<h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><ul>
<li>定义：从状态道每个动作的选择策略之间的映射。</li>
<li>数学表达：$\pi(a|s) \triangleq Pr\{A_t=a|S_t=s\}$</li>
<li>确定性策略 vs 随机性策略<ul>
<li>确定性策略：状态选择的行动是唯一的，也可以用$\pi(s)$表示。比如，状态$s$只会在$a_1, a_2, a_3$三个状态中选择$a_3$</li>
<li>随机性策略：状态选择的行动是一个概率分布，用$\pi(a|s)$表示。比如，智能体在状态s下选中$a_3$的概率是0.8，$a_2$是0.1，$a_1$是0.1</li>
<li>所以这样来看，确定性策略其实是随机性策略的一个特殊情况，也就是$a_3$被选择的概率是1，其余两个动作被选择的概率是0。<h3 id="价值函数-1"><a href="#价值函数-1" class="headerlink" title="价值函数"></a>价值函数</h3></li>
</ul>
</li>
<li>由来：从上图中我们可以发现，如果智能体的策略是随机的，那么从状态$s$转移到状态$s’$的情况就有9种（有3个动作供选择，选择一个动作后又可以跳转到3个不同的状态），还不考虑在状态$s’$之后的情况。因此，我们如果要计算$G_t$，在状态$s’$下就有$G_{t,1}, G_{t,2},\dots,G_{t,9}$，很明显，我们不可能去耗费那么多时间计算，这样就引出了价值函数的定义。 ^47f3da</li>
<li>定义：价值函数就是回报的期望值。</li>
<li>我们发现价值函数与智能体所在的状态和智能体选择的动作都有关系，由此引出下面的函数。<h4 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h4></li>
<li>定义：策略$\pi$下状态$s$的价值函数，即从状态$s$开始，智能体<strong>按照策略</strong>$\pi$进行决策所获得的回报的期望值。 </li>
<li>数学定义式：<script type="math/tex">v_{\pi}(s)\triangleq E_\pi[G_t|S_t=s]=E_{\pi}[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s]</script><h4 id="动作价值函数"><a href="#动作价值函数" class="headerlink" title="动作价值函数"></a>动作价值函数</h4></li>
<li>定义：策略$\pi$下在状态$s$时采取动作$a$的价值，即<strong>根据策略</strong>$\pi$，从状态$s$开始，执行动作$a$之后所有可能的决策序列的期望回报。</li>
<li>数学定义式：<script type="math/tex">q_{\pi}(s,a)\triangleq E_\pi[G_t|S_t=s,A_t=a]=E_{\pi}[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s,A_t=a]</script><br>注意：状态价值函数与动作$a$无关，而动作价值函数要在执行动作$a$才能确定，所以就很自然的引出了二者的关系。<h3 id="状态价值函数和动作价值函数的关系"><a href="#状态价值函数和动作价值函数的关系" class="headerlink" title="状态价值函数和动作价值函数的关系"></a>状态价值函数和动作价值函数的关系</h3>状态价值函数回溯树<br><img src="状态价值函数回溯树..jpg" alt="状态价值函数回溯树..jpg"><br>动作价值函数回溯树（以动作为$a3$为例）<br><img src="动作价值函数回溯树.jpg" alt="动作价值函数回溯树.jpg"><br>通过观察，我们其实很容易发现，$v_{\pi}(s)=\pi(a_1|s)q_{\pi}(a_1,s)+\pi(a_2|s)q_{\pi}(a_2,s)+\pi(a_1|s)q_{\pi}(a_3,s)$。<br>状态价值函数其实就是对动作价值函数求期望。推广一下，二者的关系的数学表达式为 ^48ab05<script type="math/tex; mode=display">
v_{\pi}(s)=\sum_{a \in A}\pi(a|s)q_{\pi}(a,s)</script><h2 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h2><h3 id="v-pi-s-的贝尔曼方程"><a href="#v-pi-s-的贝尔曼方程" class="headerlink" title="$v_{\pi}(s)$的贝尔曼方程"></a>$v_{\pi}(s)$的贝尔曼方程</h3></li>
</ul>
<p>^70f6d4</p>
<p>我们直接从看图手写！</p>
<ol>
<li>先推$q_\pi(a,s)$和$v_{\pi}(s)$的关系，我们首先观察$A_t$到$S_{t+1}$的切换，在切换后，在t+1时刻的收益为r，而后在状态$s’$又有回报为$G_{t+1}$，因此对于每个动作$a$都有回报<script type="math/tex; mode=display">
G_{t}=r+\gamma G_{t+1}</script>由于$v_{\pi}(s)$就是回报的期望值，所以我们可以这样理解$v_{\pi}(s)\leftrightarrow G_t$，同理，因为$s’$是状态$s$的下一个状态，所以$v_{\pi}(s’)\leftrightarrow G_{t+1}$</li>
<li>从状态$s$，通过动作$a$，转移到了状态$s’$的概率，这个概念与最开始提到的$p(s’,r|s,a)$一致</li>
<li>根据定义，$q_{\pi}(a,s)$表示执行了动作$a$之后的期望收益，所以求和概率×回报有 ^05e8c1</li>
</ol>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{aligned} 
q_{\pi}(a,s)&= \sum_{s'\in \mathcal{S}}\sum_{r \in \mathcal{R}}p(s',r|s,a)[r+\gamma v_{\pi}(s')]\\ 
\end{aligned} 
\end{equation}</script><ol>
<li>代入到二者关系的式子，就能得到$v_{\pi}$的贝尔曼方程（贝尔曼期望方程）了。<br>$$<br>\begin{equation}<br>\begin{aligned}<br>v_{\pi}(s)&amp;=\sum_{a \in A}\pi(a|s)q_{\pi}(a,s)\\<br>&amp;= \sum_{a \in A}\pi(a|s)\sum_{s’\in \mathcal{S}}\sum_{r \in \mathcal{R}}p(s’,r|s,a)[r+\gamma v_{\pi}(s’)]\\ </li>
</ol>
<p>\end{aligned}<br>\end{equation}</p>
<script type="math/tex; mode=display">
### $q_{\pi}(s,a)$的贝尔曼方程
1. 有了$v_{\pi}$的贝尔曼方程，那我们直接把它代入$v_{\pi}(s)$与$q_{\pi}(s,a)$的关系式[[#^48ab05]]，就可以得到$q_{\pi}(s,a)$的贝尔曼方程</script><p>\begin{equation}<br>\begin{aligned}<br>q_{\pi}(a,s)&amp;= \sum_{s’\in \mathcal{S}}\sum_{r \in \mathcal{R}}p(s’,r|s,a)[r+\gamma v_{\pi}(s’)]\\<br>&amp;= \sum_{s’\in \mathcal{S}}\sum_{r \in \mathcal{R}}p(s’,r|s,a)[r+\gamma \sum_{a’ \in A}\pi(a’|s’)q_{\pi}(a’,s’)]\\<br>\end{aligned}<br>\end{equation} </p>
<script type="math/tex; mode=display">
## 贝尔曼最优方程
我们已经推导出了$v_{\pi}(s)$和$q_{\pi}(s,a)$的最优方程，但我们更关注的是要找出一个策略，使得强化学习任务能够在长期过程中最大化收益。对于有限MDP，我们可以通过比较价值函数精确地定义一个最优策略。如果对于所有$s \in \mathcal{S}, \pi \ge \pi'$，那么应当有$v_{\pi}(s) \ge v_{\pi}(s')$。总会存在至少一个不比别的策略差的策略，这个策略也就是最优策略。尽管最优策略可能不止有一个，但我们还是使用$\pi_*$来表示所有这些最优策略。它们共享相同的状态价值函数，称之为最优状态价值函数，记作$v_*$，其定义为，对于任意$s\in \mathcal{S}$，</script><p>v_*(s)\doteq max_\pi v_\pi(s)</p>
<script type="math/tex; mode=display">
最优策略也共享相同的最优动作价值函数，记为$q_*$，其定义为，对于任意$s\in \mathcal{S}, a\in \mathcal{A}$,</script><p>q_*(s,a)\doteq max_\pi q_\pi(s,a)</p>
<script type="math/tex; mode=display">
上面这两个式子，也就是$v_*$和$q_*$又有什么关系呢？上面两个式子都是由最优策略$\pi_*$引出，参考上文中的回溯树[[#^48ab05]]，我们可以发现最优策略下各个状态的价值一定等于这个状态下最优动作的期望回报，也就是</script><p>v_<em>(s)=max_{a} q_{\pi_</em>}(s,a)</p>
<script type="math/tex; mode=display">
这里其实引出一个问题，为什么最优策略下各个状态的价值一定等于这个状态下最优动作的期望回报呢？我们其实可以用反过来想，如果最优策略下各个状态的价值小于这个状态下最优动作的期望回报有可能吗？这也就是说会存在一个新策略，这个新策略比最优策略更优秀，可是我们定义最优策略使最优秀的了，这就产生了矛盾。因此，上面这个式子在逻辑上是成立的。
现在我们就可以来推导二者的贝尔曼最优方程了。
### $v_*$的贝尔曼最优方程
我们参考上文的$v_\pi$的贝尔曼最优方程</script><p>v_\pi(s)=\sum_{a \in A}\pi(a|s)\sum_{s’\in \mathcal{S}}\sum_{r \in \mathcal{R}}p(s’,r|s,a)[r+\gamma v_{\pi}(s’)]</p>
<script type="math/tex; mode=display">
又因为最优策略下各个状态的价值一定等于这个状态下最优动作的期望回报，这里的最优动作其实暗含了该动作已经确定了的含义，所以$v_*$的贝尔曼最优方程为</script><p>v_<em>(s)=max_a\sum_{s’\in \mathcal{S}}\sum_{r \in \mathcal{R}}p(s’,r|s,a)[r+\gamma v_{</em>}(s’)]</p>
<script type="math/tex; mode=display">
注意，$p(s',r|s,a)$是仍然存在的，因为从状态$s$转移到下一个状态$s'$是一个概率问题，不是说我们选择了最优动作，就一定会跳转到固定的状态$s'$
### $\pi_*$的贝尔曼最优方程
我们参考上文的$q_{\pi}(s,a)$的贝尔曼方程</script><p>q_{\pi}(s,a)= \sum_{s’\in \mathcal{S}}\sum_{r \in \mathcal{R}}p(s’,r|s,a)[r+\gamma \sum_{a’ \in A}\pi(a’|s’)q_{\pi}(a’,s’)]</p>
<script type="math/tex; mode=display">
同上面推导$v_*$的贝尔曼最优方程一样，$\pi_*$的贝尔曼最优方程为</script><p>q_{<em>}(s,a)= \sum_{s’\in \mathcal{S}}\sum_{r \in \mathcal{R}}p(s’,r|s,a)[r+\gamma max_aq_{</em>}(a’,s’)]</p>
<p>$$</p>
<h2 id="部分可观察的马尔可夫决策过程-POMDP"><a href="#部分可观察的马尔可夫决策过程-POMDP" class="headerlink" title="部分可观察的马尔可夫决策过程 (POMDP"></a>部分可观察的马尔可夫决策过程 (POMDP</h2><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">Site Unreachable</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/rucwxb/p/12204045.html">【强化学习RL】必须知道的基础概念和MDP - 水奈樾 - 博客园</a><br>七元组$<S,A,T,R,\Omega,O,\gamma>$.<br>$\Omega$是Observations集合<br>$O$是有条件的observation概率的集合，也就是$O(o|s’,a)$表示在状态$s’$采取动作$a$观测到$o\in \Omega$的概率。</p>
<p><img src="comparison-table.png" alt="image-20220408144538180"><br>POMDP中，机器人不能直接确定自己处于哪个状态，只能讲，比如，在A状态的概率为0.8，在B状态的概率为0.1，在C状态的概率也为0.1。这其实就引出了belief的概念，机器人有多少的把握确定自己处在某个状态下就是belief，比如b(A)=0.8, b(B)=0.1, b(C)=0.1. 尽管如此，但是机器人可以通过某种动作，比如用传感器观察，来更新当前所处状态的可信度，也就是Observation的概念。</p>
<h2 id="置信度马尔可夫决策过程-Belief-MDP"><a href="#置信度马尔可夫决策过程-Belief-MDP" class="headerlink" title="置信度马尔可夫决策过程 (Belief MDP)"></a>置信度马尔可夫决策过程 (Belief MDP)</h2><ul>
<li>引子：具有马尔可夫性质的置信度状态可以使我们将其看作为是一个以置信度为状态的MDP.</li>
<li>特点：<ul>
<li>Belief MDP一定是定义在连续的状态空间上的，因为即便是POMDP的状态空间有限，在某个状态$s$上的概率分布是无限的，也就是说，置信度状态$b$是无限的。</li>
<li>因为智能体总是会有一些概率相信自己处在某个状态$s$上，所以对于置信度状态$b$来说，它的动作是所有状态的所有动作的总集合。</li>
</ul>
</li>
</ul>
<h2 id="置信度支持马尔可夫决策过程-Belief-Support-MDP"><a href="#置信度支持马尔可夫决策过程-Belief-Support-MDP" class="headerlink" title="置信度支持马尔可夫决策过程 (Belief Support MDP)"></a>置信度支持马尔可夫决策过程 (Belief Support MDP)</h2><p>belief support是一个集合，这个集合中包含了机器人有可能处在的状态，所以belief support MDP是有限的。</p>
<h2 id="MDP-for-multi-agents"><a href="#MDP-for-multi-agents" class="headerlink" title="MDP for multi-agents"></a>MDP for multi-agents</h2><p>给定N个智能体，多智能体的MDP为$&lt;\mathcal{S},\rho, \mathcal{A_{1, \dots, n}}, \mathcal{O_{1, \dots, n}}&gt;$.</p>
<ul>
<li>状态集合$\mathcal{S}$，其包含每个智能体的状态</li>
<li><h1 id="思考，思维流程和收获"><a href="#思考，思维流程和收获" class="headerlink" title="思考，思维流程和收获"></a>思考，思维流程和收获</h1><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3>马尔可夫决策过程中，智能体在当前时刻的状态下做出动作与环境交互，然后得到某种反馈，或者说是回报或奖励，并且进入到下一个时刻的状态。为什么强调是下一个时刻的状态呢？因为下一个时刻的状态可能与上一个时刻相同。</li>
</ul>
<p>动态特性：我认为动态特性描述的是MDP中随着时间的变化，智能体不断做出动作，获得奖励并进入下一个状态，然后不断重复这几个步骤的特点 ^ae43bb</p>
<h3 id="思维流程"><a href="#思维流程" class="headerlink" title="思维流程"></a>思维流程</h3><ol>
<li>[[#马尔可夫过程（马尔可夫链）]] -&gt; [[#马尔可夫回报过程]] -&gt; [[#马尔可夫决策过程]] -&gt; [[#]]</li>
<li>马尔可夫过程-&gt;马尔可夫回报过程-&gt;马尔可夫决策过程-&gt; POMDP-&gt; Belief MDP</li>
<li>最大化收益-&gt;回报的定义-&gt;回报会接近∞？-&gt;回报中加入折扣率-&gt;不同的状态下，选择不同的动作，回报都不一样，没法比较？-&gt;求回报的期望，也就是价值函数-&gt;状态价值函数，动作价值函数-&gt;二者关系-&gt;通过二者关系找到状态和状态的关系以及动作和动作的关系？-&gt;贝尔曼期望方程-&gt;寻求最优策略？-&gt;最优策略共享相同的两个贝尔曼最优方程<h1 id="我想不通或曾经想不通的地方"><a href="#我想不通或曾经想不通的地方" class="headerlink" title="我想不通或曾经想不通的地方"></a>我想不通或曾经想不通的地方</h1></li>
<li><p>为什么$q_{\pi}(s,a)$和$v_{\pi}(s’)$的关系的公式在《强化学习》和《深入浅出强化学习》这两本书中表达的方式不一样？这两种表达方式肯定都是对的，但是代表的是什么含义呢？</p>
</li>
<li><p>为什么也要对r求期望？</p>
</li>
</ol>
<ul>
<li>其实下图中的$R_{t+1}=r$只画了一种情况，$R_{t+1}$也是一个随机变量，它的取值是由不同的r决定的，其实从$a_3$到$s_3$的线应该有很多个不同取值的数。因此，就和上文提到的$G_t$[[#^47f3da]]一样，我们也要对r求期望。这同时也回答了问题一。表达不同的地方就是《深入浅出强化学习》用$R_s^a$代表求了期望之后的r，也就是$E[R_{t+1}|S_t=s,A_t=a]$，剩下的$E[\gamma G_{t+1}|S_t=s,A_t=a]$用了求和状态转移概率×价值函数$v_{\pi}(s’)$得到，即$\sum_{s’ \in \mathcal{S}}p(s’|s,a)[\gamma v_{\pi}(s’)] \rightarrow \gamma \sum_{s’ \in \mathcal{S}}p(s’|s,a)v_{\pi}(s’)$<br><img src="回溯树.jpg" alt="回溯树.jpg"></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p><a href="https://skywuuuu.github.io/2023/06/22/chap3-有限马尔可夫决策过程/">https://skywuuuu.github.io/2023/06/22/chap3-有限马尔可夫决策过程/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Skywuuuu</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-06-22</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-06-22</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/alipay.jpg" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/wepay.jpg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/06/21/hello/"><span class="level-item">我是谁</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpg" alt="Skywuuuu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Skywuuuu</p><p class="is-size-6 is-block">PhD. Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Category</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">0</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Skywuuuu" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Skywuuuu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://blog.csdn.net/skywuuu" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">CSDN</span></span><span class="level-right"><span class="level-item tag">blog.csdn.net</span></span></a></li><li><a class="level is-mobile" href="https://leetcode.com/Skywuuuu/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">LeetCode</span></span><span class="level-right"><span class="level-item tag">leetcode.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Personal/"><span class="level-start"><span class="level-item">Personal</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-22T01:53:38.572Z">2023-06-22</time></p><p class="title"><a href="/2023/06/22/chap3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-21T14:32:01.000Z">2023-06-21</time></p><p class="title"><a href="/2023/06/21/hello/">我是谁</a></p><p class="categories"><a href="/categories/Personal/">Personal</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/avatar.jpg" alt="Skywuuuu&#039;s personal website" height="28"></a><p class="is-size-7"><span>&copy; 2023 Skywuuuu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2023</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>